import pprint
import urllib.parse
import json5
from qwen_agent.agents import Assistant
from qwen_agent.tools.base import BaseTool, register_tool
from qwen_agent.utils.output_beautify import typewriter_print
import os
import json 
import os
import requests
from typing import Any, List, Union
from bs4 import BeautifulSoup

import re
from typing import Dict, List, Union
# Assuming BaseTool and register_tool are defined elsewhere, as in the original context.
# from some_module import BaseTool, register_tool

@register_tool('google_search', allow_overwrite=True)
class GoogleSearch(BaseTool):
    """
    A tool to search the web using the Google Custom Search API.
    """
    name = 'google_search'
    description = 'Search the web using Google Custom Search API. Returns a list of search results and urls that can be visited.'
    parameters = {
        'type': 'object',
        'properties': {
            'query': {
                'type': 'string',
                'description': 'The search query.'
            }
        },
        'required': ['query'],
    }

    def call(self, params: Union[str, dict], **kwargs) -> str:
        """
        Executes the search and returns the formatted results.
        
        Args:
            params (Union[str, dict]): The parameters for the tool, 
                                       containing the search query.
        
        Returns:
            str: A formatted string of the search results.
        """
        # Verify and extract the query from the input parameters
        params = self._verify_json_format_args(params)
        query = params['query']
        
        # Perform the search using the dedicated search method
        search_results = self.search(query)
        
        # Format the results into a string for output
        formatted_results = self._format_results(search_results)
        return formatted_results

    @staticmethod
    def search(query: str) -> List[Any]:
        """
        Performs the web search using Google Custom Search API.
        
        Args:
            query (str): The search query.
        
        Returns:
            List[Any]: A list of search result items.
            
        Raises:
            ValueError: If the required API keys are not set.
        """
        api_key = os.environ.get("GOOGLE_API_KEY")
        cse_id = os.environ.get("GOOGLE_CX")

        if not api_key or not cse_id:
            raise ValueError(
                'GOOGLE_API_KEY or GOOGLE_CX is not set! Please set them as environment variables.'
            )
            
        # Define the API endpoint and parameters
        url = "https://www.googleapis.com/customsearch/v1"
        params = {
            "key": api_key,
            "cx": cse_id,
            "q": query,
        }
        
        # Make the API request
        response = requests.get(url, params=params)
        response.raise_for_status() # Raise an exception for bad status codes (4xx or 5xx)
        
        # Return the 'items' list from the JSON response, or an empty list if not found
        return response.json().get("items", [])

    @staticmethod
    def _format_results(search_results: List[Any]) -> str:
        """
        Formats the list of search results into a single string.
        
        Args:
            search_results (List[Any]): A list of search result items from the API.
        
        Returns:
            str: A markdown-formatted string of the results.
        """
        # Create a formatted string for each result and join them together
        snippets = [
            f"[{i}] \"{doc.get('title', 'No Title')}\n{doc.get('snippet', '')}\"\nLink: {doc.get('link', '')}"
            for i, doc in enumerate(search_results, 1)
        ]
        
        if not snippets:
            return "No search results found."
            
        # Enclose the final output in a markdown code block
        return '```\n{}\n```'.format('\n\n'.join(snippets))


# Step 1 (Optional): Add a custom tool named `my_image_gen`.
@register_tool('my_image_gen')
class MyImageGen(BaseTool):
    # The `description` tells the agent the functionality of this tool.
    description = 'AI painting (image generation) service, input text description, and return the image URL drawn based on text information.'
    # The `parameters` tell the agent what input parameters the tool has.
    parameters = [{
        'name': 'prompt',
        'type': 'string',
        'description': 'Detailed description of the desired image content, in English',
        'required': True
    }]

    def call(self, params: str, **kwargs) -> str:
        # `params` are the arguments generated by the LLM agent.
        prompt = json5.loads(params)['prompt']
        prompt = urllib.parse.quote(prompt)
        return json5.dumps(
            {'image_url': f'https://image.pollinations.ai/prompt/{prompt}'},
            ensure_ascii=False)

@register_tool('web_content_extractor')
class WebContentExtractor(BaseTool):
    """
    A tool to extract the main textual content from a given URL.
    """
    name = 'web_content_extractor'
    description = 'Extracts the main content from a URL, filtering out headers, footers, ads, and navigation bars.'
    parameters = {
        'type': 'object',
        'properties': {
            'url': {
                'type': 'string',
                'description': 'The URL of the webpage to extract content from.'
            },
            'unwanted_classes': {
                'type': 'array',
                'items': {'type': 'string'},
                'description': 'A list of CSS class names to remove from the page.',
                'default': ["social", "share", "follow", "ads", "popup", "widget", "related", "sidebar", "menu"]
            },
            'unwanted_tags': {
                'type': 'array',
                'items': {'type': 'string'},
                'description': 'A list of HTML tags to remove from the page.',
                'default': ["header", "footer", "nav", "aside", "script", "style", "button", "svg"]
            },
            'wanted_tags': {
                'type': 'array',
                'items': {'type': 'string'},
                'description': 'A list of HTML tags to extract text from.',
                'default': ["h1", "h2", "h3", "p", "li", "span", "pre"]
            }
        },
        'required': ['url']
    }

    def call(self, params: Union[str, dict], **kwargs) -> str:
        """
        Main entry point for the tool.
        It parses parameters and calls the extraction logic.
        """
        params = self._verify_json_format_args(params)
        
        # Set default values from the schema if not provided
        url = params['url']
        unwanted_classes = params.get('unwanted_classes', self.parameters['properties']['unwanted_classes']['default'])
        unwanted_tags = params.get('unwanted_tags', self.parameters['properties']['unwanted_tags']['default'])
        wanted_tags = params.get('wanted_tags', self.parameters['properties']['wanted_tags']['default'])
        
        return self._extract_content(
            url=url,
            unwanted_classes=unwanted_classes,
            unwanted_tags=unwanted_tags,
            wanted_tags=wanted_tags
        )

    def _extract_content(
        self,
        url: str,
        unwanted_classes: List[str],
        unwanted_tags: List[str],
        wanted_tags: List[str],
        headers: Dict[str, str] = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}
    ) -> str:
        """
        Performs the web scraping and content extraction.
        """
        try:
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()  # Raise an exception for bad status codes (4xx or 5xx)
        except requests.RequestException as e:
            return f"Failed to fetch URL: {e}"

        soup = BeautifulSoup(response.text, "html.parser")
        
        # 1. Remove unwanted tags
        for tag in soup(unwanted_tags):
            tag.decompose()

        # 2. Remove unwanted classes
        unwanted_regex = re.compile(r"|".join(unwanted_classes), re.IGNORECASE)
        for tag in soup.find_all(class_=lambda c: c and unwanted_regex.search(c)):
            tag.decompose()
        
        # 3. Extract and format content from wanted tags
        paragraphs = []
        for p in soup.find_all(wanted_tags):
            text = p.get_text(strip=True)
            if not text:
                continue

            if p.name == "h1":
                content = f"tittle: {text}."
            elif p.name == "h2":
                content = f"sub-tittle: {text}."
            elif p.name == "h3":
                content = f"subsection-tittle: {text}."
            elif p.name == "li":
                content = f"- {text}"
            else: # p, span, pre
                content = text
            
            paragraphs.append(content)
            
        return " ".join(paragraphs) if paragraphs else "No content extracted."

@register_tool('file_saver')
class FileSaverTool(BaseTool):
    """
    A tool to save a content to a local file.
    """
    name = 'file_saver'
    description = 'Saves content to a file with a specified name, extension, and location.'
    parameters = {
        'type': 'object',
        'properties': {
            'filename': {
                'type': 'string',
                'description': 'The name of the file, without the extension.'
            },
            'content': {
                'type': 'string',
                'description': 'The string content to write to the file.'
            },
            'extension': {
                'type': 'string',
                'description': 'The file extension (e.g., ".txt", ".py").',
                'default': '.txt'
            },
            'location': {
                'type': 'string',
                'description': 'The directory path where the file will be saved.',
                'default': './'
            }
        },
        'required': ['filename', 'content']
    }

    def call(self, params: Union[str, dict], **kwargs) -> Dict[str, str]:
        """
        Main entry point for the tool.
        Parses parameters and calls the file-saving logic.
        """
        params = self._verify_json_format_args(params)
        
        filename = params['filename']
        content = params['content']
        # Use .get() to safely access optional parameters with their defaults
        extension = params.get('extension', self.parameters['properties']['extension']['default'])
        location = params.get('location', self.parameters['properties']['location']['default'])
        
        return self._save_to_file(
            filename=filename,
            content=content,
            extension=extension,
            location=location
        )

    def _save_to_file(self, filename: str, content: str, extension: str, location: str) -> Dict[str, str]:
        """
        Handles the file writing operation with error handling.
        """
        # Ensure the directory exists
        try:
            if not os.path.exists(location):
                os.makedirs(location)
        except OSError as e:
            return {
                "status": "error",
                "message": f"Failed to create directory {location}: {e}"
            }
            
        # Construct the full file path
        file_path = os.path.join(location, f"{filename}{extension}")

        # Write the content to the file
        try:
            with open(file_path, "w", encoding="utf-8") as file:
                file.write(content)
            
            print(f"File created successfully at {file_path}")
            return {
                "status": "success",
                "message": f"File created successfully at {file_path}",
            }
        except IOError as e:
            print(f"Error writing to file: {e}")
            return {
                "status": "error",
                "message": f"Failed to write to file {file_path}: {e}",
            }

# Step 2: Configure the LLM you are using.
llm_cfg = {
    # Use the model service provided by DashScope:
    #'model': 'qwen3-235b-a22b-thinking-2507',
    # 'model_type': 'qwen_dashscope',
   # 'api_key': 'sk-a98b266a5d0747aead3d90957752356b',
    # It will use the `DASHSCOPE_API_KEY' environment variable if 'api_key' is not set here.

    # Use a model service compatible with the OpenAI API, such as vLLM or Ollama:
     'model':'qwen/qwen3-14b', #'qwen/qwen3-4b-thinking-2507',
    'model_server': 'http://192.168.5.108:1234/v1/',#'https://dashscope-intl.aliyuncs.com/compatible-mode/v1', # 'http://127.0.0.1:1234/v1/', #,  # base_url, also known as api_base
    # 'api_key': 'lm-studio',

    # (Optional) LLM hyperparameters for generation:    
}

# Step 3: Create an agent. Here we use the `Assistant` agent as an example, which is capable of using tools and reading files.
system_instruction = """
You are Aria, a world-class, meticulous deep research agent. Your identity is defined by your adherence to a strict, evidence-based protocol.

Your sole purpose is to produce comprehensive, accurate, and deeply-sourced reports. Your primary function is real-world data extraction and synthesis. Any deviation from this protocol, especially the fabrication of data or simulation of tool use, is a critical failure of your core directive and will terminate the process.

---

### ### CORE DIRECTIVES & PROTOCOL ###

1.  **Internal Monologue (`<thinking>`):** You MUST use `<thinking>` and `</thinking>` tags to contain your entire internal process. This includes your planning, reasoning, justification for actions, tool calls, and real-time management of your Knowledge Log. This is your auditable work log.

2.  **Final Output:** The user must ONLY see the final, polished report. The report must be generated *after* all research is complete and must exist entirely outside of the `<thinking>` tags.

3.  **CRITICAL DIRECTIVE: NO SIMULATION.** You must perform every tool call for real. **Simulation, hallucination, or fabrication of URLs, quotes, or any data is strictly forbidden.** You must ground your work in the real data returned by the tools. To enforce this, after every tool call, you must explicitly state what the tool returned (e.g., "Google Search returned 10 results." or "web_extractor successfully retrieved 4,500 words of text from URL X.").

---

### ### NON-NEGOTIABLE THREE-PHASE WORKFLOW ###
You must execute the following three phases in strict sequential order.

### PHASE 1: RECONNAISSANCE & STRATEGY
1.  **Initial Deconstruction:** Analyze the user's query and formulate an initial set of 1-3 distinct, targeted search queries to cover different angles of the topic. Justify each query in your internal monologue.
2.  **Iterative Reconnaissance Loop (Max 3 Searches):**
    a. Execute the most promising `Google Search` query.
    b. **VERIFY THE OUTPUT.** Briefly analyze the search results (titles and snippets). In your `<thinking>` block, determine if the results are relevant and high-quality.
    c. **DECIDE:** If the results are poor, discard the query, state why, and execute the next planned query. If the results are good, you may stop the loop early. You have a maximum of 3 search attempts to gather a high-quality set of initial sources.
3.  **Log Initialization & Population:**
    a. Create a "Knowledge Log" with these columns:
       `| URL | Status | Justification for Selection | Proof of Extraction (Quote or Data) | Key Findings | Contradictions/Gaps |`
    b. Select the 5-7 most promising and diverse URLs from your successful search(es).
    c. Populate the `URL` and `Justification for Selection` columns. The justification should briefly explain why this source seems valuable based on its title/snippet (e.g., "Official company report," "In-depth analysis from a reputable news source").
    d. Set the `Status` for each URL to `Pending Extraction`.

### PHASE 2: DEEP EXTRACTION & ANALYSIS
1.  **Systematic Extraction:** Sequentially iterate through each URL in the Knowledge Log with a `Pending Extraction` status.
2.  **CRITICAL DIRECTIVE: MANDATORY FULL-CONTENT RETRIEVAL.** For each URL, you **MUST** use `web_extractor` or `doc_parser` to retrieve its **FULL** content.
3.  **Verification and Analysis:**
    a. **PROOF OF WORK:** In your `<thinking>` block, you MUST confirm the tool call was successful and state the approximate size of the content retrieved (e.g., "web_extractor returned a 3,200-word article."). If a tool fails (e.g., 404 error, paywall), you MUST log this failure in the `Key Findings` column and move to the next URL.
    b. **Analyze the Real Content:** Carefully read and analyze the *actual text* returned by the tool.
    c. **Populate the Log:**
        i. `Proof of Extraction (Quote or Data)`: Populate with **one direct, insightful quote** or a key data point from the retrieved content to prove you have analyzed it.
        ii. `Key Findings` & `Contradictions/Gaps`: Synthesize the core information and populate these columns.
4.  **Update Status:** Once a URL is fully processed, update its `Status` to `Complete` or `Extraction Failed`.
5.  **CONSTRAINT:** The `Google Search` tool is **FORBIDDEN** in this phase.

### PHASE 3: SYNTHESIS & REPORTING
1.  **Holistic Review:** Once all possible URLs are processed, conduct a final review of the entire Knowledge Log. Cross-reference all findings, patterns, and contradictions.
2.  **Gap Analysis & Final Search (Conditional):** Ask yourself: "Does a critical, unresolvable gap exist that prevents me from fully answering the user's query?"
    - **ONLY IF** such a gap is identified, you are permitted to perform **one final, hyper-targeted `Google Search`**. You must explicitly justify this exceptional search in your `<thinking>` block and process the new source(s) via Phase 2.
3.  **Report Generation:** Synthesize a comprehensive, well-structured report based **exclusively** on the aggregated information within your Knowledge Log. Do not introduce outside information.
4.  **Citations & Integrity:** You **MUST** cite your sources (the URLs) for all major claims. If you found conflicting information, you **MUST** highlight this conflict in your final report.
"""
print(system_instruction)



tools = ['my_image_gen', 'code_interpreter', 'web_content_extractor', 'google_search', 'doc_parser', 'file_saver']  # `code_interpreter` is a built-in tool for executing code.
files = ['./examples/resource/doc.pdf']  # Give the bot a PDF file to read.
bot = Assistant(llm=llm_cfg,
                system_message=system_instruction,
                function_list=tools,
                )

from qwen_agent.gui import WebUI
WebUI(bot).run()  # bot is the agent defined in the above code, we do not repeat the definition here for saving space.


"""
# Step 4: Run the agent as a chatbot.
messages = []  # This stores the chat history.
while True:
    # For example, enter the query "draw a dog and rotate it 90 degrees".
    query = input('\nuser query: ')
    # Append the user query to the chat history.
    messages.append({'role': 'user', 'content': query})
    response = []
    response_plain_text = ''
    print('bot response:')
    for response in bot.run(messages=messages):
        # Streaming output.
        response_plain_text = typewriter_print(response, response_plain_text)
    # Append the bot responses to the chat history.
    messages.extend(response)

"""
