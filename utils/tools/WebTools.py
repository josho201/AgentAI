
from qwen_agent.tools.base import BaseTool, register_tool

import requests
import re
import urllib.parse
import json5
from typing import List, Union, Any, Dict
from bs4 import BeautifulSoup
import os 
import dotenv

@register_tool('google_search', allow_overwrite=True)
class GoogleSearch(BaseTool):
    """
    A tool to search the web using the Google Custom Search API.
    """
    name = 'google_search'
    description = 'Search the web using Google Custom Search API. Returns a list of search results and urls that can be visited.'
    parameters = {
        'type': 'object',
        'properties': {
            'query': {
                'type': 'string',
                'description': 'The search query.'
            }
        },
        'required': ['query'],
    }

    def call(self, params: Union[str, dict], **kwargs) -> str:
        """
        Executes the search and returns the formatted results.
        
        Args:
            params (Union[str, dict]): The parameters for the tool, 
                                       containing the search query.
        
        Returns:
            str: A formatted string of the search results.
        """
        # Verify and extract the query from the input parameters
        params = self._verify_json_format_args(params)
        query = params['query']
        
        # Perform the search using the dedicated search method
        search_results = self.search(query)
        
        # Format the results into a string for output
        formatted_results = self._format_results(search_results)
        return formatted_results

    @staticmethod
    def search(query: str) -> List[Any]:
        """
        Performs the web search using Google Custom Search API.
        
        Args:
            query (str): The search query.
        
        Returns:
            List[Any]: A list of search result items.
            
        Raises:
            ValueError: If the required API keys are not set.
        """
        api_key = os.getenv("GOOGLE_API_KEY")
        cse_id = os.getenv("GOOGLE_CX")

        if not api_key or not cse_id:
            raise ValueError(
                'GOOGLE_API_KEY or GOOGLE_CX is not set! Please set them as environment variables.'
            )
            
        # Define the API endpoint and parameters
        url = "https://www.googleapis.com/customsearch/v1"
        params = {
            "key": api_key,
            "cx": cse_id,
            "q": query,
        }
        
        # Make the API request
        response = requests.get(url, params=params)
        response.raise_for_status() # Raise an exception for bad status codes (4xx or 5xx)
        
        # Return the 'items' list from the JSON response, or an empty list if not found
        return response.json().get("items", [])

    @staticmethod
    def _format_results(search_results: List[Any]) -> str:
        """
        Formats the list of search results into a single string.
        
        Args:
            search_results (List[Any]): A list of search result items from the API.
        
        Returns:
            str: A markdown-formatted string of the results.
        """
        # Create a formatted string for each result and join them together
        snippets = [
            f"[{i}] \"{doc.get('title', 'No Title')}\n{doc.get('snippet', '')}\"\nLink: {doc.get('link', '')}"
            for i, doc in enumerate(search_results, 1)
        ]
        
        if not snippets:
            return "No search results found."
            
        # Enclose the final output in a markdown code block
        return '```\n{}\n```'.format('\n\n'.join(snippets))


# Step 1 (Optional): Add a custom tool named `my_image_gen`.
@register_tool('my_image_gen')
class MyImageGen(BaseTool):
    # The `description` tells the agent the functionality of this tool.
    description = 'AI painting (image generation) service, input text description, and return the image URL drawn based on text information.'
    # The `parameters` tell the agent what input parameters the tool has.
    parameters = [{
        'name': 'prompt',
        'type': 'string',
        'description': 'Detailed description of the desired image content, in English',
        'required': True
    }]

    def call(self, params: str, **kwargs) -> str:
        # `params` are the arguments generated by the LLM agent.
        prompt = json5.loads(params)['prompt']
        prompt = urllib.parse.quote(prompt)
        return json5.dumps(
            {'image_url': f'https://image.pollinations.ai/prompt/{prompt}'},
            ensure_ascii=False)

@register_tool('web_content_extractor')
class WebContentExtractor(BaseTool):
    """
    A tool to extract the main textual content from a given URL.
    """
    name = 'web_content_extractor'
    description = 'Extracts the main content from a URL, filtering out headers, footers, ads, and navigation bars.'
    parameters = {
        'type': 'object',
        'properties': {
            'url': {
                'type': 'string',
                'description': 'The URL of the webpage to extract content from.'
            },
            'unwanted_classes': {
                'type': 'array',
                'items': {'type': 'string'},
                'description': 'A list of CSS class names to remove from the page.',
                'default': ["social", "share", "follow", "ads", "popup", "widget", "related", "sidebar", "menu"]
            },
            'unwanted_tags': {
                'type': 'array',
                'items': {'type': 'string'},
                'description': 'A list of HTML tags to remove from the page.',
                'default': ["header", "footer", "nav", "aside", "script", "style", "button", "svg"]
            },
            'wanted_tags': {
                'type': 'array',
                'items': {'type': 'string'},
                'description': 'A list of HTML tags to extract text from.',
                'default': ["h1", "h2", "h3", "p", "li", "span", "pre"]
            }
        },
        'required': ['url']
    }

    def call(self, params: Union[str, dict], **kwargs) -> str:
        """
        Main entry point for the tool.
        It parses parameters and calls the extraction logic.
        """
        params = self._verify_json_format_args(params)
        
        # Set default values from the schema if not provided
        url = params['url']
        unwanted_classes = params.get('unwanted_classes', self.parameters['properties']['unwanted_classes']['default'])
        unwanted_tags = params.get('unwanted_tags', self.parameters['properties']['unwanted_tags']['default'])
        wanted_tags = params.get('wanted_tags', self.parameters['properties']['wanted_tags']['default'])
        
        return self._extract_content(
            url=url,
            unwanted_classes=unwanted_classes,
            unwanted_tags=unwanted_tags,
            wanted_tags=wanted_tags
        )

    def _extract_content(
        self,
        url: str,
        unwanted_classes: List[str],
        unwanted_tags: List[str],
        wanted_tags: List[str],
        headers: Dict[str, str] = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}
    ) -> str:
        """
        Performs the web scraping and content extraction.
        """
        try:
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()  # Raise an exception for bad status codes (4xx or 5xx)
        except requests.RequestException as e:
            return f"Failed to fetch URL: {e}"

        soup = BeautifulSoup(response.text, "html.parser")
        
        # 1. Remove unwanted tags
        for tag in soup(unwanted_tags):
            tag.decompose()

        # 2. Remove unwanted classes
        unwanted_regex = re.compile(r"|".join(unwanted_classes), re.IGNORECASE)
        for tag in soup.find_all(class_=lambda c: c and unwanted_regex.search(c)):
            tag.decompose()
        
        # 3. Extract and format content from wanted tags
        paragraphs = []
        for p in soup.find_all(wanted_tags):
            text = p.get_text(strip=True)
            if not text:
                continue

            if p.name == "h1":
                content = f"tittle: {text}."
            elif p.name == "h2":
                content = f"sub-tittle: {text}."
            elif p.name == "h3":
                content = f"subsection-tittle: {text}."
            elif p.name == "li":
                content = f"- {text}"
            else: # p, span, pre
                content = text
            
            paragraphs.append(content)
            
        return " ".join(paragraphs) if paragraphs else "No content extracted."
