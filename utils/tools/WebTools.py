
from qwen_agent.tools.base import BaseTool, register_tool

import requests
import re
import urllib.parse
import json5
from typing import List, Union, Any, Dict
from bs4 import BeautifulSoup
import os 
from dotenv import load_dotenv
load_dotenv()

@register_tool('google_search', allow_overwrite=True)
class GoogleSearch(BaseTool):
    """
    A tool to search the web using the Google Custom Search API.
    """
    name = 'google_search'
    description = 'Search the web using Google Custom Search API. Returns a list of search results and urls that can be visited.'
    parameters = {
        'type': 'object',
        'properties': {
            'query': {
                'type': 'string',
                'description': 'The search query.'
            }
        },
        'required': ['query'],
    }

    def call(self, params: Union[str, dict], **kwargs) -> str:
        """
        Executes the search and returns the formatted results.
        
        Args:
            params (Union[str, dict]): The parameters for the tool, 
                                       containing the search query.
        
        Returns:
            str: A formatted string of the search results.
        """
        # Verify and extract the query from the input parameters
        params = self._verify_json_format_args(params)
        query = params['query']
        
        # Perform the search using the dedicated search method
        search_results = self.search(query)
        
        # Format the results into a string for output
        formatted_results = self._format_results(search_results)
        return formatted_results

    @staticmethod
    def search(query: str) -> List[Any]:
        """
        Performs the web search using Google Custom Search API.
        
        Args:
            query (str): The search query.
        
        Returns:
            List[Any]: A list of search result items.
            
        Raises:
            ValueError: If the required API keys are not set.
        """
        api_key = os.getenv("GOOGLE_API_KEY")
        cse_id = os.getenv("GOOGLE_CX")

        if not api_key or not cse_id:
            raise ValueError(
                'GOOGLE_API_KEY or GOOGLE_CX is not set! Please set them as environment variables.'
            )
            
        # Define the API endpoint and parameters
        url = "https://www.googleapis.com/customsearch/v1"
        params = {
            "key": api_key,
            "cx": cse_id,
            "q": query,
        }
        
        # Make the API request
        response = requests.get(url, params=params)
        response.raise_for_status() # Raise an exception for bad status codes (4xx or 5xx)
        
        # Return the 'items' list from the JSON response, or an empty list if not found
        return response.json().get("items", [])

    @staticmethod
    def _format_results(search_results: List[Any]) -> str:
        """
        Formats the list of search results into a single string.
        
        Args:
            search_results (List[Any]): A list of search result items from the API.
        
        Returns:
            str: A markdown-formatted string of the results.
        """
        # Create a formatted string for each result and join them together
        snippets = [
            f"[{i}] \"{doc.get('title', 'No Title')}\n{doc.get('snippet', '')}\"\nLink: {doc.get('link', '')}"
            for i, doc in enumerate(search_results, 1)
        ]
        
        if not snippets:
            return "No search results found."
            
        # Enclose the final output in a markdown code block
        return '```\n{}\n```'.format('\n\n'.join(snippets))


# Step 1 (Optional): Add a custom tool named `my_image_gen`.
@register_tool('my_image_gen')
class MyImageGen(BaseTool):
    # The `description` tells the agent the functionality of this tool.
    description = 'AI painting (image generation) service, input text description, and return the image URL drawn based on text information.'
    # The `parameters` tell the agent what input parameters the tool has.
    parameters = [{
        'name': 'prompt',
        'type': 'string',
        'description': 'Detailed description of the desired image content, in English',
        'required': True
    }]

    def call(self, params: str, **kwargs) -> str:
        # `params` are the arguments generated by the LLM agent.
        prompt = json5.loads(params)['prompt']
        prompt = urllib.parse.quote(prompt)
        return json5.dumps(
            {'image_url': f'https://image.pollinations.ai/prompt/{prompt}'},
            ensure_ascii=False)

@register_tool('web_content_extractor')
class WebContentExtractor(BaseTool):
    """
    A tool to extract the main textual content from a given URL.
    """
    name = 'web_content_extractor'
    description = "Retrieves the main text content from a given webpage URL. Supports extracting a specific character range, allowing partial reads or continuation from a chosen point."

    tags = {
            'unwanted_classes':["social", "share", "follow", "ads", "popup", "widget", "related", "sidebar", "menu"]            ,
            'unwanted_tags':["header", "footer", "nav", "aside", "script", "style", "button", "svg"],
            'wanted_tags': ["h1", "h2", "h3", "p", "li", "span", "pre"]
        }
    
    parameters = {
        'type': 'object',
        'properties': {
            'url': {
                'type': 'string',
                'description': 'The URL of the webpage to extract content from.'
            },
            'begin': {
                'type': 'integer',
                'description': 'The starting character index of the content to extract.',
               'default': 0
            },
            'end': {
                'type': 'integer',
                'description': 'The ending character index of the content to extract.',
               'default': 4000
            }
        },
        'required': ['url']
    }

    def call(self, params: Union[str, dict], **kwargs) -> str:
        """
        Main entry point for the tool.
        It parses parameters and calls the extraction logic.
        """
        
        # Set default values from the schema if not provided

        params = json5.loads(params) if isinstance(params, str) else params

        url = params.get('url', "google.com")
        begin = params.get('begin', self.parameters['properties']['begin']['default'])
        end =params.get('end', self.parameters['properties']['end']['default'])

        unwanted_classes = self.tags['unwanted_classes'] # params.get('unwanted_classes', self.parameters['properties']['unwanted_classes']['default'])
        unwanted_tags =   self.tags['unwanted_tags'] #params.get('unwanted_tags', self.parameters['properties']['unwanted_tags']['default'])
        wanted_tags = self.tags["wanted_tags"] #params.get('wanted_tags', self.parameters['properties']['wanted_tags']['default'])
        
        return self._extract_content(
            url=url,
            begin=begin,
            end=end
        )

    def _extract_content(
        self,
        url: str,
        begin: int = 0,
        end: int = 4000,
        headers: Dict[str, str] = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}
    ) -> str:    
        try:
            headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"}
            response = requests.get(url, headers=headers, timeout=15)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "html.parser")

            for tag in soup(["style", "script", "nav", "header", "footer", "img", "video"]):
                tag.decompose()


            unwanted_regex = re.compile(r"|".join(["footer"]), re.IGNORECASE)
            for tag in soup.find_all(class_=lambda c: c and unwanted_regex.search(c)):
                tag.decompose()
            
            # --- PASO 3: EXTRAER - Recorrer los elementos y convertirlos a Markdown ---
            paragraphs = []
            for p in soup.find_all(["table", "h1", "h2", "h3", "p", "li", "pre", "blockquote"], recursive=True):
                # Ignoramos si el elemento está dentro de una tabla (ya que table_to_markdown se encarga)
                if p.find_parent("table"):
                    if p.name != "table": # Evita procesar 'p' dentro de una 'td' de una tabla ya procesada
                        continue

                text = p.get_text(strip=False) # strip=True es mejor para limpiar espacios en blanco
                if not text:
                    continue
                
                # Tu lógica de conversión, ligeramente ajustada para consistencia
                if p.name == "h1":
                    content = f"# {text.strip()}\n"
                elif p.name == "h2":
                    content = f"## {text.strip()}\n"
                elif p.name == "h3":
                    content = f"### {text.strip()}\n"
                elif p.name == "li":
                    content = f"- {text.strip()}\n"
                elif p.name == "pre" or p.name == "blockquote":
                    content = f"> {text.strip()}\n"
                elif p.name == "table":
                    content = self._table_to_markdown(p)
                else: # p
                    content = f"{text}\n"
                
                paragraphs.append(content)

            return "".join(paragraphs)[begin:end] if paragraphs else "Error: No se pudo extraer contenido significativo."

        except requests.exceptions.RequestException as e:
            return f"Error al acceder a la URL: {e}"
        except Exception as e:
            return f"Ocurrió un error inesperado: {e}"
        

    def _table_to_markdown(self, table_tag):
        processed_rows = []
        table_header = table_tag.find('th')
        title = f"### {table_header.get_text(strip=True)}\n\n" if table_header else ""
        for row in table_tag.find_all('tr'):
            cols = row.select(':scope > td')
            if not cols and row.find('th'):
                cols = row.find_all('td')
            if len(cols) < 2:
                continue
            key = cols[0].get_text(strip=True)
            value = cols[1].get_text(strip=True)
            if key:
                processed_rows.append([key, value])
            elif processed_rows:
                processed_rows[-1][1] += f" / {value}"
        if not processed_rows:
            return title
        markdown_table = f"{title}| Característica | Detalles |\n"
        markdown_table += "|---|---|\n"
        for key, value in processed_rows:
            key_clean = key.replace('|', r'\|')
            value_clean = value.replace('|', r'\|')
            markdown_table += f"| {key_clean} | {value_clean} |\n"
        return markdown_table.join("\n\n")

